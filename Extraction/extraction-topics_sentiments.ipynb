{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Detection and Sentiment Analysis\n",
    "### Part 1: Exploration of POS and DEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_transformers\n",
    "import spacy.displacy as displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "document = nlp('''\n",
    "Given that it’s backed up by solid performance and fair price, there’s nothing wrong with that at all.\n",
    "''')\n",
    "displacy.render(document, style=\"dep\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Rules for Topic Detection\n",
    "- How do we find topics mentioned?\n",
    "- Can we extract subsentences (a.k.a. clauses) related to each topic?\n",
    "\n",
    "Note: in the solution below we only follow direct left and right children. We could probably improve our approach by following them recursively. Remember recursion? :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_detection(document):\n",
    "    result = {}\n",
    "\n",
    "    topics = []\n",
    "    for token in document:\n",
    "        if ((token.pos_ == \"NOUN\") and \n",
    "            (token.dep_ == \"nsubj\" or \n",
    "             token.dep_ == \"pobj\" or \n",
    "             token.dep_ == \"conj\")):\n",
    "            topics.append(token)\n",
    "\n",
    "    for topic in topics:\n",
    "        subsentence = []\n",
    "        for lefty in topic.lefts:\n",
    "            subsentence.append(lefty.text)\n",
    "        subsentence.append(topic.text)\n",
    "        for righty in topic.rights:\n",
    "            subsentence.append(righty.text)\n",
    "\n",
    "        result[topic.text] = \" \".join(subsentence)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_detection(document))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Sentiment Analysis\n",
    "\n",
    "Note: we now start with another document to better demonstrate the kinds of results obtained when doing sentiment analysis. The following would typically we applied to each row of a dataframe containing our original texts. As a result we obtain 2 new dataframes: one with the sentiments per sentence, another one with the sentiments per subsentence (as detected by our approach in part 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "document = nlp('''I am not sure if this is the case all around the \n",
    "    world, but in American sports culture, fans love to argue not just \n",
    "    which athlete is better, but also which has been properly rated. \n",
    "    The gist is that being good isn't enough, the athlete has to be at \n",
    "    least as good or better as the hype bestowed on them (usually by \n",
    "    media or apparel companies like Nike). Not living up to the hype \n",
    "    results in the dreaded overrated moniker, and the athlete usually \n",
    "    receives criticism or mockery.''')\n",
    "sentence_df = pd.DataFrame(columns=[\n",
    "    \"sentence\", \n",
    "    \"tb_polarity\", \n",
    "    \"tb_subjectivity\", \n",
    "    \"sia_neg\", \n",
    "    \"sia_neu\", \n",
    "    \"sia_pos\", \n",
    "    \"sia_compound\"\n",
    "    ])\n",
    "subsentence_df = pd.DataFrame(columns=[\n",
    "    \"sentence\", \n",
    "    \"topic\", \n",
    "    \"subsentence\", \n",
    "    \"tb_polarity\", \n",
    "    \"tb_subjectivity\", \n",
    "    \"sia_neg\", \n",
    "    \"sia_neu\", \n",
    "    \"sia_pos\", \n",
    "    \"sia_compound\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "i = 0\n",
    "for sentence in document.sents:\n",
    "    sentence_sentiment_tb = TextBlob(sentence.text).sentiment\n",
    "    sentence_sentiment_sia = sia.polarity_scores(sentence.text)\n",
    "    new_row = pd.DataFrame({\n",
    "        \"sentence\": sentence.text,\n",
    "        \"tb_polarity\": sentence_sentiment_tb.polarity,\n",
    "        \"tb_subjectivity\": sentence_sentiment_tb.subjectivity,\n",
    "         \"sia_neg\": sentence_sentiment_sia[\"neg\"],\n",
    "         \"sia_neu\": sentence_sentiment_sia[\"neu\"],\n",
    "         \"sia_pos\": sentence_sentiment_sia[\"pos\"],\n",
    "         \"sia_compound\": sentence_sentiment_sia[\"compound\"],\n",
    "    }, index=[i])\n",
    "    sentence_df = pd.concat([new_row, sentence_df])\n",
    "\n",
    "    topics = topic_detection(nlp(sentence.text))\n",
    "    for topic, subsentence in topics.items():\n",
    "        subsentence_sentiment_tb = TextBlob(subsentence).sentiment\n",
    "        subsentence_sentiment_sia = sia.polarity_scores(subsentence)\n",
    "        new_row = pd.DataFrame({\n",
    "        \"sentence\": sentence.text,\n",
    "        \"topic\": topic,\n",
    "        \"subsentence\": subsentence,\n",
    "        \"tb_polarity\": subsentence_sentiment_tb.polarity,\n",
    "        \"tb_subjectivity\": subsentence_sentiment_tb.subjectivity,\n",
    "         \"sia_neg\": subsentence_sentiment_sia[\"neg\"],\n",
    "         \"sia_neu\": subsentence_sentiment_sia[\"neu\"],\n",
    "         \"sia_pos\": subsentence_sentiment_sia[\"pos\"],\n",
    "         \"sia_compound\": subsentence_sentiment_sia[\"compound\"],\n",
    "    }, index=[i])\n",
    "    subsentence_df = pd.concat([new_row, subsentence_df])\n",
    "\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsentence_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
