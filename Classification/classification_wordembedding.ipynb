{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Using Word Embedding\n",
    "\n",
    "- Obtaining and loading data\n",
    "- Exploring the data\n",
    "- Machine learning\n",
    "    - Split data\n",
    "    - Create word embedding\n",
    "    - Vectorisation: average word embedding per document\n",
    "    - Model fiting\n",
    "    - Model evaluation\n",
    "- Apply model (do one prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/mental_health.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "df[\"label\"].reset_index().groupby(\"label\").count().plot(\n",
    "    kind=\"barh\", legend=False, ax=ax).grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "def clean(text, stopwords):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "\n",
    "    text_list = text.split()\n",
    "    text_list = [word for word in text_list if word not in stopwords]\n",
    "\n",
    "    lematizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    text_list = [lematizer.lemmatize(word) for word in text_list]\n",
    "\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "df[\"text_clean\"] = df[\"text\"].apply(\n",
    "    lambda x:\n",
    "        clean(x, stopwords)\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lambda_reset = lambda x: x.reset_index()\n",
    "df_train, df_test = [\n",
    "    lambda_reset(item) for item in train_test_split(df, test_size=0.2)]\n",
    "\n",
    "y_train = df_train[\"label\"].values\n",
    "y_test = df_test[\"label\"].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding: Do the following only to save embedding to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.phrases\n",
    "\n",
    "def create_list_corpus(column):\n",
    "    # create list of lists of unigrams\n",
    "    result = []\n",
    "    for string in column:\n",
    "        list_of_words = string.split()\n",
    "        list_of_unigrams = [\" \".join(list_of_words[i:i+1])\n",
    "                            for i in range(0, len(list_of_words), 1)]\n",
    "        result.append(list_of_unigrams)\n",
    "\n",
    "    ## detect bigrams and trigrams\n",
    "    bigrams_detector = gensim.models.phrases.Phrases(result, \n",
    "                  delimiter=\" \", min_count=5, threshold=10)\n",
    "    bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "    trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[result], \n",
    "               delimiter=\" \", min_count=5, threshold=10)\n",
    "    trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
    "\n",
    "    return result\n",
    "\n",
    "list_corpus_train = create_list_corpus(df_train[\"text_clean\"])\n",
    "list_corpus_test = create_list_corpus(df_test[\"text_clean\"])\n",
    "\n",
    "# just to get a useful window size for the embedding model\n",
    "avg_length = 0 if len(list_corpus_train) == 0 else sum(\n",
    "        [len(member) for member in list_corpus_train]\n",
    "    ) / len(list_corpus_train)\n",
    "print(\n",
    "    \"Nof: \", len(list_corpus_train), \n",
    "    \"Max: \", max([len(member) for member in list_corpus_train]), \n",
    "    \"Min: \", min([len(member) for member in list_corpus_train]), \n",
    "    \"Avg: \", avg_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "embedding = Word2Vec(window=72, workers=6, epochs=20, sg=1)\n",
    "embedding.build_vocab(list_corpus_train, progress_per=100)\n",
    "embedding.train(\n",
    "    list_corpus_train, \n",
    "    total_examples=embedding.corpus_count, \n",
    "    epochs=embedding.epochs)\n",
    "\n",
    "embedding.save(\"results/embedding.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "\n",
    "word = \"happy\"\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "## word embedding\n",
    "tot_words = [word] + [tupla[0] for tupla in \n",
    "                 embedding.wv.most_similar(word, topn=21)]\n",
    "X = embedding.wv[tot_words]\n",
    "## pca to reduce dimensionality from 100 to 3\n",
    "pca = manifold.TSNE(perplexity=12, n_components=3, init='pca')\n",
    "X = pca.fit_transform(X)\n",
    "## create dtf\n",
    "dtf_ = pd.DataFrame(X, index=tot_words, columns=[\"x\",\"y\",\"z\"])\n",
    "## plot 3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(dtf_['x'], \n",
    "           dtf_['y'], \n",
    "           dtf_['z'], c=\"black\")\n",
    "           \n",
    "for label, row in dtf_[[\"x\",\"y\",\"z\"]].iterrows():\n",
    "    x, y, z = row\n",
    "    ax.text(x, y, z, s=label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here on: work with saved embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "embedding_from_disk = Word2Vec.load(\"results/embedding.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_embedding(embedding, individual_text):\n",
    "    words = [word for word in individual_text if \n",
    "                word in embedding.wv.key_to_index]\n",
    "    if len(words) > 0:\n",
    "        return np.mean(embedding.wv[words], axis=0)\n",
    "    else:\n",
    "        print(\"> Empty :-( \", individual_text)\n",
    "        return np.zeros(embedding.wv.vector_size) # TODO useful behaviour?\n",
    "\n",
    "print(df_train.loc[0].at[\"text_clean\"])\n",
    "print(average_embedding(\n",
    "    embedding_from_disk, df_train.loc[0].at[\"text_clean\"]))\n",
    "print(y_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_awes = [average_embedding(embedding_from_disk, current) \n",
    "                for current in df_train[\"text_clean\"]]\n",
    "X_test_awes = [average_embedding(embedding_from_disk, current) \n",
    "               for current in df_test[\"text_clean\"]]\n",
    "\n",
    "print(X_train_awes[:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_awes, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predictions = model.predict(X_test_awes)\n",
    "accuracy = metrics.accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "confusion = metrics.confusion_matrix(y_true=y_test, y_pred=predictions)\n",
    "print(accuracy)\n",
    "print(confusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_of_text = [average_embedding(\n",
    "    embedding_from_disk, df_train.loc[0].at[\"text_clean\"])]\n",
    "print(average_of_text)\n",
    "a = model.predict(average_of_text)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b02d1009969c06045bd3fe2072aa85171b8afb02d21440c6833c7c84721fd1f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
